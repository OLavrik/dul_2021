{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Homework10_selfsupervised.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd1123e5b7f344c89b3fa3bc11ae7e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bdcd3897de874f3e8c98b9e0a9525042",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_764b4b2de3824cc3b8afee6a920d3aff",
              "IPY_MODEL_ce32adf52c9e4522aede31c8c7250065",
              "IPY_MODEL_13ca672fbee345bfa9f3f83fcd88f84b"
            ]
          }
        },
        "bdcd3897de874f3e8c98b9e0a9525042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "764b4b2de3824cc3b8afee6a920d3aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b1303f523dc45a2aa5009eaff6e7244",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epochs:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a4ad4a8bb1a4ec6af6e6ba17a098fc8"
          }
        },
        "ce32adf52c9e4522aede31c8c7250065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dff97c991d394c41905eeb17c2117a5f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5543ebd56ab40c7bb4021e50d1d47dd"
          }
        },
        "13ca672fbee345bfa9f3f83fcd88f84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d5f0b82720e435eb3e3b1ffbc6917c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdbe80b9193b41fa89e28f76370c71b6"
          }
        },
        "7b1303f523dc45a2aa5009eaff6e7244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a4ad4a8bb1a4ec6af6e6ba17a098fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dff97c991d394c41905eeb17c2117a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5543ebd56ab40c7bb4021e50d1d47dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d5f0b82720e435eb3e3b1ffbc6917c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdbe80b9193b41fa89e28f76370c71b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99ec41d62df142cf8c33a1d1ff0da7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e45ae35fd8214fd98d8c4267f43256a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14657e2be0644c3690daf0c6c35d601c",
              "IPY_MODEL_1981860c27e24051a7e69493eed0fcb0",
              "IPY_MODEL_6b5b0aea0cd74f9f931c3522948c9124"
            ]
          }
        },
        "e45ae35fd8214fd98d8c4267f43256a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14657e2be0644c3690daf0c6c35d601c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da31b46fe9084eb9b2f0bc827c6ac619",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Training:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2794a18798ee4297b068f5e859af1c9d"
          }
        },
        "1981860c27e24051a7e69493eed0fcb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7e7c98f33845448ca840edc7c5a15c46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 469,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f325cea84b28401199875293da8da42d"
          }
        },
        "6b5b0aea0cd74f9f931c3522948c9124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f1eabe53b27443b9ab45f467a97b67ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/469 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc1adf8b447b4008940bcb905076db65"
          }
        },
        "da31b46fe9084eb9b2f0bc827c6ac619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2794a18798ee4297b068f5e859af1c9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e7c98f33845448ca840edc7c5a15c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f325cea84b28401199875293da8da42d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1eabe53b27443b9ab45f467a97b67ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc1adf8b447b4008940bcb905076db65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZT2VPDHkxZa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de218a0b-9d8d-43f8-f0d5-596aca899c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dul_2021'...\n",
            "remote: Enumerating objects: 339, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 339 (delta 96), reused 80 (delta 62), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (339/339), 55.24 MiB | 12.45 MiB/s, done.\n",
            "Resolving deltas: 100% (157/157), done.\n",
            "Processing ./dul_2021\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: dul-2021\n",
            "  Building wheel for dul-2021 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=25374 sha256=34872cea0f02e7cbf83cede4c42bfefbfc6da6d9e615092e226e6d2a76523b3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3c_ocfau/wheels/55/59/29/0fb1c635652157734f4d741f32fc11979149684e83e919de06\n",
            "Successfully built dul-2021\n",
            "Installing collected packages: dul-2021\n",
            "Successfully installed dul-2021-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
        "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
        "!pip install ./dul_2021"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dul_2021.utils.hw10_utils import *"
      ],
      "metadata": {
        "id": "kvPNepG2L2Hp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1. Context Encoder"
      ],
      "metadata": {
        "id": "86iMf3mEPaCR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRyHjNsSDzPY"
      },
      "source": [
        "Here we will implement [context encoder](https://arxiv.org/abs/1604.07379). The context encoder structures its self-supervised learning task by inpainting masked images. For example, the figure below shows different masking shapes, such as center masking, random block masking, and segmentation masking. Note that segmentation masking (c) is not purely self-supervised since we would need to train a image segmentation model which requires labels. However, the other two masking schemes (a) and (b) and purely self-supervised.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fhzkULYTtyMGUUF2n9dlPayJSdcY5pRv)\n",
        "\n",
        "More formally, the context encoder optimizes the following reconstruction loss:\n",
        "$$\\mathcal{L}_{rec} = \\left\\Vert \\hat{M} \\odot (x - F((1 - \\hat{M})\\odot x)) \\right\\Vert^2_2$$\n",
        "where $\\hat{M}$ is the masked region, $x$ is the image, and $F$ is the context encoder that tries to reconstruct the masked portion. In addition to the reconstruction loss, the paper introduces an adversarial loss that encourages more realistic inpaintings.\n",
        "$$L_{adv} = \\max_D \\mathbb{E}_{x\\in \\chi} [\\log(D(x)) + \\log(1 - D(F((1-\\hat{M})\\odot x)))]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task we will crop central 14x14 region. You can use slightly afjusted architectures from AVB task from homework 8.\n",
        "\n",
        "**Hyperparametrs**\n",
        "\n",
        "* latent_dim = 128\n",
        "* epochs ~ 10-20\n",
        "* classifier need fewer updates than encoder-decoder part. We suggest to update it on each 10-th iteration.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record the mse loss and adversarial losses per batch.\n",
        "3. 30 (1, 28, 28) images. Where first 10 images are random sample from testdata with removed central region. Next 10 images are reconstracted images with your trained model. Last 10 images are initial without any removal."
      ],
      "metadata": {
        "id": "uW2dsZBKEo8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# curl http://localhost:5001/api/code_search/check_project/ --header \"Content-Type: application/json\" --request POST --data '{\"project_name\":\"meow\"}'\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from itertools import chain\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ContextDataset(Dataset):\n",
        "    _img_h, _img_w = 28, 28\n",
        "    _crop_h, _crop_w = 14, 14\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self._data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # [1; h; w]\n",
        "        image, _ = self._data[idx]\n",
        "        # [1; h; w]\n",
        "        mask = self.get_mask()\n",
        "        target = image[mask].reshape(-1, self._crop_h, self._crop_w)\n",
        "        image[mask] = 0\n",
        "        return image, target\n",
        "\n",
        "    @classmethod\n",
        "    def get_mask(cls):\n",
        "        mask = torch.zeros((1, cls._img_h, cls._img_w), dtype=torch.bool)\n",
        "        h_c, w_c = cls._img_h // 2, cls._img_w // 2\n",
        "        h_p, w_p = cls._crop_h // 2, cls._crop_w // 2\n",
        "        mask[:, h_c - h_p: h_c + h_p, w_c - w_p: w_c + w_p] = True\n",
        "        return mask\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def create_mask(images_b, crop_height=14, crop_width=14):\n",
        "    print(images_b.shape)\n",
        "    tg=deepcopy(images_b)\n",
        "    for i, images in enumerate(images_b):\n",
        "        # images=images_b[i]\n",
        "        mask = torch.zeros((1, 28, 28), dtype=torch.bool)\n",
        "        h_c, w_c = 28 // 2, 28 // 2\n",
        "        h_p, w_p = 28 // 2, 28 // 2\n",
        "        mask[:, h_c - h_p: h_c + h_p, w_c - w_p: w_c + w_p] = True\n",
        "        target = images[mask].reshape(-1, crop_height, crop_width)\n",
        "        images[mask] = 0\n",
        "        tg[i]=target\n",
        "        images_b[i]=images\n",
        "\n",
        "    return images_b, target\n",
        "\n",
        "    # img = []\n",
        "    # tg = []\n",
        "    # for image in images:\n",
        "    #     mask = torch.zeros((1, 28, 28), dtype=torch.bool)\n",
        "    #     h_c, w_c = 28 // 2, 28 // 2\n",
        "    #     h_p, w_p = 28 // 2, 28 // 2\n",
        "    #     mask[:, h_c - h_p: h_c + h_p, w_c - w_p: w_c + w_p] = True\n",
        "    #     target = image[mask].reshape(-1, crop_height, crop_width)\n",
        "    #     image[mask] = 0\n",
        "    #     img.append(image)\n",
        "    #     tg.append(target)\n",
        "    # img = np.asarray(img)\n",
        "    # tg = np.asarray(tg)\n",
        "    # return torch.from_numpy(img), torch.from_numpy(tg)\n",
        "\n",
        "\n",
        "class DenoisingBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_ch: int,\n",
        "            out_ch: int,\n",
        "            stride: int = 1,\n",
        "            bias: bool = False,\n",
        "            upsample: bool = False,\n",
        "            lr_cf: float = 0.2,\n",
        "            dropout: float = 0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, (3, 3), stride=stride, padding=1, bias=bias)\n",
        "        self.norm = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.LeakyReLU(lr_cf)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n",
        "        x = x + torch.randn_like(x) * 0.05\n",
        "        x = self.dropout(x)\n",
        "        return self.act(self.norm(self.conv(x)))\n",
        "\n",
        "\n",
        "class MaskedImageEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            DenoisingBlock(1, 16, stride=2),  # [1; 28; 28] -> [16; 14; 14]\n",
        "            DenoisingBlock(16, 32, stride=2),  # [16; 14; 14] -> [32; 7; 7]\n",
        "            DenoisingBlock(32, 64, stride=2),  # [32; 7; 7] -> [64; 4; 4]\n",
        "            DenoisingBlock(64, 128, stride=2),  # [64; 4; 4] -> [128; 2; 2]\n",
        "            DenoisingBlock(128, 128, stride=2).conv,  # [128; 2; 2] -> [128; 1; 1]\n",
        "        )\n",
        "        self.linear = nn.Linear(128, latent_dim)\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        convolved = self.blocks(images)\n",
        "        return self.linear(convolved.view(images.shape[0], -1))\n",
        "\n",
        "\n",
        "class PatchDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(latent_dim, 128 * 2 * 2)\n",
        "        self.blocks = nn.Sequential(\n",
        "            DenoisingBlock(128, 64, upsample=True),  # [128; 2; 2] -> [64; 4; 4]\n",
        "            DenoisingBlock(64, 32, upsample=True),  # [64; 4; 4] -> [32; 8; 8]\n",
        "            DenoisingBlock(32, 16, upsample=True),  # [32; 8; 8] -> [16; 16; 16]\n",
        "            nn.Conv2d(16, 1, kernel_size=(3, 3)),  # [16; 16; 16] -> [1; 14; 14]\n",
        "        )\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        sized = self.linear(images)\n",
        "        sized = sized.view(-1, 128, 2, 2)\n",
        "        decoded = self.blocks(sized)\n",
        "        return torch.tanh(decoded)\n",
        "\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            DenoisingBlock(1, 16, stride=2),  # [1; 14; 14] -> [16; 7; 7]\n",
        "            DenoisingBlock(16, 32, stride=2),  # [16; 7; 7] -> [32; 7; 7]\n",
        "            DenoisingBlock(32, 64, stride=2),  # [32; 4; 4] -> [64; 2; 2]\n",
        "            DenoisingBlock(64, 128, stride=2),  # [64; 2; 2] -> [128; 1; 1]\n",
        "        )\n",
        "        self.linear = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        convolved = self.blocks(images)\n",
        "        logit = self.linear(convolved.view(images.shape[0], -1))\n",
        "        return torch.sigmoid(logit)\n",
        "\n",
        "\n",
        "class ContextEncoder:\n",
        "    def __init__(self, latent_dim: int, device: torch.device):\n",
        "        self.encoder = MaskedImageEncoder(latent_dim).to(device)\n",
        "        self.decoder = PatchDecoder(latent_dim).to(device)\n",
        "        self.discriminator = PatchDiscriminator().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def _train_epoch(self, train_dataloader: DataLoader, ed_optim: Optimizer, d_optim: Optimizer):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        self.discriminator.train()\n",
        "\n",
        "        rec_losses = []\n",
        "        adv_losses = []\n",
        "        postfix = {}\n",
        "        data_pbar = tqdm(train_dataloader, leave=False, desc=\"Training\", postfix=postfix)\n",
        "        for images in data_pbar:\n",
        "            images, patches = create_mask(images)\n",
        "            images = images.to(self.device)\n",
        "            patches = patches.to(self.device)\n",
        "            bs = images.shape[0]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.encoder(images)\n",
        "                reconstruction = self.decoder(embeddings)\n",
        "\n",
        "            real_patches = self.discriminator(patches)\n",
        "            fake_patches = self.discriminator(reconstruction)\n",
        "\n",
        "            adv_loss = F.binary_cross_entropy(real_patches, torch.ones((bs, 1), device=self.device))\n",
        "            adv_loss += F.binary_cross_entropy(fake_patches, torch.zeros((bs, 1), device=self.device))\n",
        "\n",
        "            d_optim.zero_grad()\n",
        "            adv_loss.backward()\n",
        "            d_optim.step()\n",
        "\n",
        "            embeddings = self.encoder(images)\n",
        "            reconstruction = self.decoder(embeddings)\n",
        "            mse_loss = F.mse_loss(reconstruction, patches)\n",
        "\n",
        "            ed_optim.zero_grad()\n",
        "            mse_loss.backward()\n",
        "            ed_optim.step()\n",
        "\n",
        "            adv_losses.append(adv_loss.item())\n",
        "            rec_losses.append(mse_loss.item())\n",
        "            postfix = {\"mse\": rec_losses[-1], \"adv\": adv_losses[-1]}\n",
        "            data_pbar.set_postfix(postfix)\n",
        "        data_pbar.close()\n",
        "\n",
        "        return rec_losses, adv_losses\n",
        "\n",
        "    def train(self, train_dataloader: DataLoader, n_epochs: int, lr: float):\n",
        "        ed_optim = torch.optim.AdamW(chain(self.encoder.parameters(), self.decoder.parameters()), lr=lr)\n",
        "        d_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=lr)\n",
        "\n",
        "        rec_losses = []\n",
        "        adv_losses = []\n",
        "\n",
        "        epoch_bar = trange(n_epochs, desc=\"Epochs\")\n",
        "        for _ in epoch_bar:\n",
        "            epoch_rec_loss, epoch_adv_losses = self._train_epoch(train_dataloader, ed_optim, d_optim)\n",
        "            rec_losses += epoch_rec_loss\n",
        "            adv_losses += epoch_adv_losses\n",
        "        epoch_bar.close()\n",
        "        return rec_losses, adv_losses\n",
        "\n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "        self.discriminator.eval()\n"
      ],
      "metadata": {
        "id": "DuY4sicMAAIe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from itertools import chain\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm, trange\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def create_mask(images_b, crop_height=14, crop_width=14):\n",
        "    print(images_b.shape)\n",
        "    tg=torch.zeros((images_b.shape[0], 14, 14))\n",
        "    for i, images in enumerate(images_b):\n",
        "        mask = torch.zeros((1, 28, 28), dtype=torch.bool)\n",
        "        h_c, w_c = 28 // 2, 28 // 2\n",
        "        h_p, w_p = 14 // 2, 14 // 2\n",
        "        mask[:, h_c - h_p: h_c + h_p, w_c - w_p: w_c + w_p] = True\n",
        "        target = images[mask].reshape(-1, crop_height, crop_width)\n",
        "        images[mask] = 0\n",
        "        tg[i]=target\n",
        "        images_b[i]=images\n",
        "    return images_b, tg\n",
        "\n",
        "\n",
        "\n",
        "class DenoisingBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_ch: int,\n",
        "            out_ch: int,\n",
        "            stride: int = 1,\n",
        "            bias: bool = False,\n",
        "            upsample: bool = False,\n",
        "            lr_cf: float = 0.2,\n",
        "            dropout: float = 0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, (3, 3), stride=stride, padding=1, bias=bias)\n",
        "        self.norm = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.LeakyReLU(lr_cf)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n",
        "        x = x + torch.randn_like(x) * 0.05\n",
        "        x = self.dropout(x)\n",
        "        return self.act(self.norm(self.conv(x)))\n",
        "\n",
        "\n",
        "class MaskedImageEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            DenoisingBlock(1, 16, stride=2),  # [1; 28; 28] -> [16; 14; 14]\n",
        "            DenoisingBlock(16, 32, stride=2),  # [16; 14; 14] -> [32; 7; 7]\n",
        "            DenoisingBlock(32, 64, stride=2),  # [32; 7; 7] -> [64; 4; 4]\n",
        "            DenoisingBlock(64, 128, stride=2),  # [64; 4; 4] -> [128; 2; 2]\n",
        "            DenoisingBlock(128, 128, stride=2).conv,  # [128; 2; 2] -> [128; 1; 1]\n",
        "        )\n",
        "        self.linear = nn.Linear(128, latent_dim)\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        convolved = self.blocks(images)\n",
        "        return self.linear(convolved.view(images.shape[0], -1))\n",
        "\n",
        "\n",
        "class PatchDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(latent_dim, 128 * 2 * 2)\n",
        "        self.blocks = nn.Sequential(\n",
        "            DenoisingBlock(128, 64, upsample=True),  # [128; 2; 2] -> [64; 4; 4]\n",
        "            DenoisingBlock(64, 32, upsample=True),  # [64; 4; 4] -> [32; 8; 8]\n",
        "            DenoisingBlock(32, 16, upsample=True),  # [32; 8; 8] -> [16; 16; 16]\n",
        "            nn.Conv2d(16, 1, kernel_size=(3, 3)),  # [16; 16; 16] -> [1; 14; 14]\n",
        "        )\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        sized = self.linear(images)\n",
        "        sized = sized.view(-1, 128, 2, 2)\n",
        "        decoded = self.blocks(sized)\n",
        "        return torch.tanh(decoded)\n",
        "\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            DenoisingBlock(1, 16, stride=2),  # [1; 14; 14] -> [16; 7; 7]\n",
        "            DenoisingBlock(16, 32, stride=2),  # [16; 7; 7] -> [32; 7; 7]\n",
        "            DenoisingBlock(32, 64, stride=2),  # [32; 4; 4] -> [64; 2; 2]\n",
        "            DenoisingBlock(64, 128, stride=2),  # [64; 2; 2] -> [128; 1; 1]\n",
        "        )\n",
        "        self.linear = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        convolved = self.blocks(images)\n",
        "        logit = self.linear(convolved.view(images.shape[0], -1))\n",
        "        return torch.sigmoid(logit)\n",
        "\n",
        "\n",
        "class ContextEncoder:\n",
        "    def __init__(self, latent_dim: int, device: torch.device):\n",
        "        self.encoder = MaskedImageEncoder(latent_dim).to(device)\n",
        "        self.decoder = PatchDecoder(latent_dim).to(device)\n",
        "        self.discriminator = PatchDiscriminator().to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def _train_epoch(self, train_dataloader: DataLoader, ed_optim: Optimizer, d_optim: Optimizer):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        self.discriminator.train()\n",
        "\n",
        "        rec_losses = []\n",
        "        adv_losses = []\n",
        "        postfix = {}\n",
        "        data_pbar = tqdm(train_dataloader, leave=False, desc=\"Training\", postfix=postfix)\n",
        "        for images in data_pbar:\n",
        "            images, patches = create_mask(images)\n",
        "            images = images.to(self.device)\n",
        "            patches = patches.to(self.device)\n",
        "            bs = images.shape[0]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.encoder(images)\n",
        "                reconstruction = self.decoder(embeddings)\n",
        "\n",
        "            real_patches = self.discriminator(patches)\n",
        "            fake_patches = self.discriminator(reconstruction)\n",
        "\n",
        "            adv_loss = F.binary_cross_entropy(real_patches, torch.ones((bs, 1), device=self.device))\n",
        "            adv_loss += F.binary_cross_entropy(fake_patches, torch.zeros((bs, 1), device=self.device))\n",
        "\n",
        "            d_optim.zero_grad()\n",
        "            adv_loss.backward()\n",
        "            d_optim.step()\n",
        "\n",
        "            embeddings = self.encoder(images)\n",
        "            reconstruction = self.decoder(embeddings)\n",
        "            mse_loss = F.mse_loss(reconstruction, patches)\n",
        "\n",
        "            ed_optim.zero_grad()\n",
        "            mse_loss.backward()\n",
        "            ed_optim.step()\n",
        "\n",
        "            adv_losses.append(adv_loss.item())\n",
        "            rec_losses.append(mse_loss.item())\n",
        "            postfix = {\"mse\": rec_losses[-1], \"adv\": adv_losses[-1]}\n",
        "            data_pbar.set_postfix(postfix)\n",
        "        data_pbar.close()\n",
        "\n",
        "        return rec_losses, adv_losses\n",
        "\n",
        "    def train(self, train_dataloader: DataLoader, n_epochs: int, lr: float):\n",
        "        ed_optim = torch.optim.AdamW(chain(self.encoder.parameters(), self.decoder.parameters()), lr=lr)\n",
        "        d_optim = torch.optim.AdamW(self.discriminator.parameters(), lr=lr)\n",
        "\n",
        "        rec_losses = []\n",
        "        adv_losses = []\n",
        "\n",
        "        epoch_bar = trange(n_epochs, desc=\"Epochs\")\n",
        "        for _ in epoch_bar:\n",
        "            epoch_rec_loss, epoch_adv_losses = self._train_epoch(train_dataloader, ed_optim, d_optim)\n",
        "            rec_losses += epoch_rec_loss\n",
        "            adv_losses += epoch_adv_losses\n",
        "        epoch_bar.close()\n",
        "        return rec_losses, adv_losses\n",
        "\n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "        self.discriminator.eval()\n"
      ],
      "metadata": {
        "id": "YDSlrNGZBVYj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q1(train_data, test_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "    test_data: An (n_test, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array of full of mse losses\n",
        "    - a (# of training iterations, ) numpy array of full of adversarial losses\n",
        "    - a (30, 1, 28, 28) numpy array of 10 transformed images, 10 reconstructions, and 10 groundtruths\n",
        "    \"\"\"\n",
        "    \n",
        "    train_data = train_data.data.unsqueeze(1).float()\n",
        "    train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True, pin_memory=True)\n",
        "\n",
        "    context_encoder = ContextEncoder(10, device)\n",
        "    mse_losses, adv_losses = context_encoder.train(train_dataloader, n_epochs=1, lr=1e-3)\n",
        "\n",
        "    return None, None, None"
      ],
      "metadata": {
        "id": "U9AObhZW5BCZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_results(q1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "cd1123e5b7f344c89b3fa3bc11ae7e24",
            "bdcd3897de874f3e8c98b9e0a9525042",
            "764b4b2de3824cc3b8afee6a920d3aff",
            "ce32adf52c9e4522aede31c8c7250065",
            "13ca672fbee345bfa9f3f83fcd88f84b",
            "7b1303f523dc45a2aa5009eaff6e7244",
            "7a4ad4a8bb1a4ec6af6e6ba17a098fc8",
            "dff97c991d394c41905eeb17c2117a5f",
            "c5543ebd56ab40c7bb4021e50d1d47dd",
            "2d5f0b82720e435eb3e3b1ffbc6917c9",
            "cdbe80b9193b41fa89e28f76370c71b6",
            "99ec41d62df142cf8c33a1d1ff0da7b0",
            "e45ae35fd8214fd98d8c4267f43256a4",
            "14657e2be0644c3690daf0c6c35d601c",
            "1981860c27e24051a7e69493eed0fcb0",
            "6b5b0aea0cd74f9f931c3522948c9124",
            "da31b46fe9084eb9b2f0bc827c6ac619",
            "2794a18798ee4297b068f5e859af1c9d",
            "7e7c98f33845448ca840edc7c5a15c46",
            "f325cea84b28401199875293da8da42d",
            "f1eabe53b27443b9ab45f467a97b67ae",
            "bc1adf8b447b4008940bcb905076db65"
          ]
        },
        "id": "F6yl4YrT5Geo",
        "outputId": "fe7ee0c2-b1a8-460b-c0a2-22ddd190c72e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd1123e5b7f344c89b3fa3bc11ae7e24",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99ec41d62df142cf8c33a1d1ff0da7b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-5f2b2c180ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq1_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/dul_2021/utils/hw10_utils.py\u001b[0m in \u001b[0;36mq1_results\u001b[0;34m(q1)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mq1_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0maeloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclfloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mplot_ce_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maeloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclfloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-49f0b3c62c67>\u001b[0m in \u001b[0;36mq1\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcontext_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContextEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmse_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-24d0adb8af14>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, n_epochs, lr)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mepoch_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mepoch_rec_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_adv_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0med_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_optim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0mrec_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepoch_rec_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0madv_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepoch_adv_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-24d0adb8af14>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_dataloader, ed_optim, d_optim)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mreal_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mfake_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-24d0adb8af14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mconvolved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvolved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-24d0adb8af14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 1, 3, 3], but got 3-dimensional input of size [128, 14, 14] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c7OSiq3ytd85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2. Rotations Prediction"
      ],
      "metadata": {
        "id": "W6LVLHMMjzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will imlement this [paper](https://arxiv.org/abs/1803.07728). Here, model learns good representations for downstream tasks by proxy task of prediciting rotation of the original image.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1eHXLH-N_6uMGRzdf1Wjnga26qlS5-FRv)\n",
        "\n",
        "We will work with same rotations as in paper (0, 90, 180, 270). You can use architecture AVB task in hw8. Latent dim 128 and 10 epochs should be enough.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record the loss per batch.\n",
        "2. Over the course of training, record the accuracy score for each iteration.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TUP2DAeeIK0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q2(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array of full of losses\n",
        "    - a (# of training epochs, ) numpy array of full of accuracy scores\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "_YSch41UKy2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_results(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "VIpLkvcU39ib",
        "outputId": "ce73f4ac-53e6-4fcf-ed8b-e0b47be5cd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV9b3/8dcHCKCCoBBRQQ0qLhTXArXuu6DWvbfSVrF1+WmrV2uvvVhbrXZx4dat1Vq3KnXDtaWC4C4oCoR9h7CHNSwBwpL18/vjzDk5OScJScjkBOb9fDzy4MxyZj4zJPOZ7zLfMXdHRESiq0WmAxARkcxSIhARiTglAhGRiFMiEBGJOCUCEZGIUyIQEYk4JQIRkYhTIpDIM7PFZnZupuMQyRQlAhGRiFMiEKmGmbUxs8fNbEXw87iZtQmWdTaz982s0MzWm9kYM2sRLPtfM1tuZpvNbK6ZnRPMb2Fmg8xsgZmtM7M3zWzfYFlbM3slmF9oZhPMrEvmjl6iRolApHr3ACcBxwPHAX2B3wTLfgnkA9lAF+DXgJvZkcCtQB93bw9cACwOvnMbcBlwBnAgsAF4Klg2EOgAHAR0Am4GtoV3aCJVKRGIVO9HwAPuvsbdC4D7gWuCZaXAAcAh7l7q7mM8NmhXOdAG6GlmWe6+2N0XBN+5GbjH3fPdvRj4HXCVmbUKttcJONzdy919ortvarIjlchTIhCp3oHAkqTpJcE8gMFAHvChmS00s0EA7p4H3EHsIr/GzN4ws/h3DgHeC6p+CoHZxBJHF+CfwCjgjaAa6hEzywr38EQqKRGIVG8FsYt33MHBPNx9s7v/0t0PBS4B7oy3Bbj7a+5+avBdBx4Ovr8M6O/uHZN+2rr78qBUcb+79wROBi4Grm2SoxRBiUAkLitotG1rZm2B14HfmFm2mXUG7gVeATCzi83scDMzYCOxO/sKMzvSzM4OGpW3E6vnrwi2/wzwRzM7JNhGtpldGnw+y8yOMbOWwCZiVUUViDQRJQKRmBHELtzxn7ZALjANmA5MAv4QrNsD+BgoAr4Gnnb3z4i1DzwErAVWAfsBdwffeQIYRqw6aTPwDfCdYNn+wNvEksBs4Ati1UUiTcL0YhoRkWhTiUBEJOKUCEREIk6JQEQk4pQIREQirlWmA6ivzp07e05OTqbDEBHZpUycOHGtu2dXt2yXSwQ5OTnk5uZmOgwRkV2KmS2paZmqhkREIk6JQEQk4pQIREQibpdrIxARaQylpaXk5+ezffv2TIfSqNq2bUu3bt3Iyqr7ALahJQIze5HYKIpr3L1XLev1ITZey9Xu/nZY8YiIJMvPz6d9+/bk5OQQGz9w1+furFu3jvz8fLp3717n74VZNfQS0K+2FYLRFh8GPgwxDhGRNNu3b6dTp067TRIAMDM6depU71JOaInA3UcD63ew2m3AO8CasOIQEanJ7pQE4hpyTBlrLDazrsDlwN/qsO5NZpZrZrkFBQUN2t+cVZsYPGoOG7aUNOj7IiK7q0z2Gnoc+F933+ELONz9WXfv7e69s7OrfTBuhxav3cpTny1geaHeCS4izUO7du0yHQKQ2V5DvYm9oxWgM3ChmZW5+7/C2Fnndq0BWKcSgYhIFRkrEbh7d3fPcfccYm9n+llYSQCgfdtYV6qi7WVh7UJEpEHcnbvuuotevXpxzDHHMHToUABWrlzJ6aefzvHHH0+vXr0YM2YM5eXlXHfddYl1H3vssZ3ef5jdR18HzgQ6m1k+cB+QBeDuz4S135q0zYrlvO2l5U29axFp5u7/z0xmrdjUqNvseeDe3Pe9b9Vp3XfffZcpU6YwdepU1q5dS58+fTj99NN57bXXuOCCC7jnnnsoLy9n69atTJkyheXLlzNjxgwACgsLdzrW0BKBuw+ox7rXhRVHXIugJb1cr+YUkWbmyy+/ZMCAAbRs2ZIuXbpwxhlnMGHCBPr06cNPf/pTSktLueyyyzj++OM59NBDWbhwIbfddhsXXXQR559//k7vPzJPFrdsEUsEFRVKBCJSVV3v3Jva6aefzujRoxk+fDjXXXcdd955J9deey1Tp05l1KhRPPPMM7z55pu8+OKLO7WfyIw1FE8EKhGISHNz2mmnMXToUMrLyykoKGD06NH07duXJUuW0KVLF2688UZuuOEGJk2axNq1a6moqODKK6/kD3/4A5MmTdrp/UemRBCvGlKJQESam8svv5yvv/6a4447DjPjkUceYf/99+fll19m8ODBZGVl0a5dO4YMGcLy5cv5yU9+QkVFrOf9gw8+uNP7j0wiSJQIlAhEpJkoKioCYk8DDx48mMGDB1dZPnDgQAYOHJj2vcYoBSSLTtVQorE4w4GIiDQzkUkELYIjVdWQiEhVkUkEaiwWkVS+G14PGnJMkUkEiecIVCIQEWIvcFm3bt1ulQzi7yNo27Ztvb4XucZiVQ2JCEC3bt3Iz8+noSMaN1fxN5TVR2QSQaL7qPKAiABZWVn1eovX7ixCVUOxfx1lAhGRZJFJBKYSgYhItSKTCBJ2o4YhEZHGEKlEYIYqhkREUkQrEaACgYhIqmglAjM1FouIpIhWIkAlAhGRVNFKBGojEBFJE61EgKlEICKSIrREYGYvmtkaM5tRw/Ifmdk0M5tuZmPN7LiwYqncqR4oExFJFWaJ4CWgXy3LFwFnuPsxwO+BZ0OMBYi1ESgPiIhUFdpYQ+4+2sxyalk+NmnyG6B+oyQ1gNoIRETSNZc2guuBD2paaGY3mVmumeXuzEiBsTYCpQIRkWQZTwRmdhaxRPC/Na3j7s+6e293752dnd3gfbUwdR8VEUmV0WGozexY4Hmgv7uva4L9adA5EZEUGSsRmNnBwLvANe4+r0n2iXoNiYikCq1EYGavA2cCnc0sH7gPyAJw92eAe4FOwNPBENFl7t47rHhiQalqSEQkVZi9hgbsYPkNwA1h7b861pQ7ExHZRWS8sbgpmanXkIhIqoglAj1HICKSKlqJALURiIikilYi0PsIRETSRCsRoBKBiEiqaCUCtRGIiKSJVCJA7yMQEUkTqURgGodaRCRNpBJBC4OKikxHISLSvEQqERjqNSQikipaiUBjDYmIpIlWIkAtBCIiqaKVCEy9hkREUkUqEYDeRyAikipSicBUNyQikiZyiUB5QESkqmglAvQ+AhGRVNFKBCoRiIikiVYiQM8RiIikilQiKNxWyupN2zMdhohIsxJaIjCzF81sjZnNqGG5mdmTZpZnZtPM7MSwYokr3FrKuEXrw96NiMguJcwSwUtAv1qW9wd6BD83AX8LMRYREalBaInA3UcDtd1+XwoM8ZhvgI5mdkBY8YiISPUy2UbQFViWNJ0fzEtjZjeZWa6Z5RYUFDRJcCIiUbFLNBa7+7Pu3tvde2dnZ2c6HBGR3UomE8Fy4KCk6W7BPBERaUKZTATDgGuD3kMnARvdfWUG4xERiaRWYW3YzF4HzgQ6m1k+cB+QBeDuzwAjgAuBPGAr8JOwYhERkZqFlgjcfcAOljvw87D2LyIidbNLNBaLiEh4lAhERCJOiUBEJOIimQjKyisyHYKISLMRyUSgkahFRCpFMxEoE4iIJEQzEahMICKSEM1EoDwgIpIQyUQgIiKVIpkIVCIQEakUyURQoUwgIpIQyUSgNCAiUimaiUAlAhGRhGgmgkwHICLSjEQzEWiECRGRhGgmApUJREQSopkIlAdERBKimQgyHYCISDMSyUTw+vilmQ5BRKTZCDURmFk/M5trZnlmNqia5Qeb2WdmNtnMppnZhWHG07KFAbBgTVGYuxER2aWElgjMrCXwFNAf6AkMMLOeKav9BnjT3U8ArgaeDisegK4d9wBUNSQikizMEkFfIM/dF7p7CfAGcGnKOg7sHXzuAKwIMR4sViDQEBMiIknCTARdgWVJ0/nBvGS/A35sZvnACOC26jZkZjeZWa6Z5RYUFDQ4oBZBJlAeEBGplOnG4gHAS+7eDbgQ+KeZpcXk7s+6e293752dnd3gnalEICKSLsxEsBw4KGm6WzAv2fXAmwDu/jXQFugcVkBBHlCJQEQkSZiJYALQw8y6m1lrYo3Bw1LWWQqcA2BmRxNLBA2v+9kBi1cNqblYRCQhtETg7mXArcAoYDax3kEzzewBM7skWO2XwI1mNhV4HbjOQxwatEW8akhjDYmIJLQKc+PuPoJYI3DyvHuTPs8CTgkzhmTxxmK1EYiIVMp0Y3FGKA2IiFSqUyIws73ivXnM7Agzu8TMssINrfGp+6iISLq6lghGA23NrCvwIXAN8FJYQYUl3n1UbygTEalU10Rg7r4VuAJ42t2/D3wrvLDCoTYCEZF0dU4EZvZd4EfA8GBey3BCCk+815DSgIhIpbomgjuAu4H3gi6ghwKfhRdWSBIlggzHISLSjNSp+6i7fwF8ARA0Gq919/8OM7AwVD5ZrEwgIhJX115Dr5nZ3ma2FzADmGVmd4UbWuNLVA0pD4iIJNS1aqinu28CLgM+ALoT6zm0S1FjsYhIuromgqzguYHLgGHuXsou2OZqKhGIiKSpayL4O7AY2AsYbWaHAJvCCiosGnRORCRdXRuLnwSeTJq1xMzOCiek8CQGnVMeEBFJqGtjcQczezT+ljAz+zOx0sEuxYgPMaFMICISV9eqoReBzcB/BT+bgH+EFVRYWgRHqzwgIlKprsNQH+buVyZN329mU8IIKEzxEoF6DYmIVKpriWCbmZ0anzCzU4Bt4YQUHlMbgYhImrqWCG4GhphZh2B6AzAwnJDCc9cFRzJm/lqu/Ha3TIciItJs1LXX0FTgODPbO5jeZGZ3ANPCDK6x7bNnawDatIrk+3hERKpVryuiu28KnjAGuDOEeEJlicGGMhqGiEizsjO3xrbjVZoXPVAmIpJuZxLBDq+mZtbPzOaaWZ6ZDaphnf8ys1lmNtPMXtuJeHaocvTRMPciIrJrqbWNwMw2U/0F34A9dvDdlsBTwHlAPjDBzIa5+6ykdXoQe8/BKe6+wcz2q2f89WJ6MY2ISJpaE4G7t9+JbfcF8tx9IYCZvQFcCsxKWudG4Cl33xDsb81O7G+HKp8sDnMvIiK7ljC7z3QFliVN5wfzkh0BHGFmX5nZN2bWr7oNmdlN8eEtCgoKGhxQZYlAmUBEJC7T/ShbAT2AM4EBwHNm1jF1JXd/1t17u3vv7OzsBu9MbQQiIunCTATLgYOSprsF85LlE7zfwN0XAfOIJYZwqI1ARCRNmIlgAtDDzLqbWWvgamBYyjr/IlYawMw6E6sqWhhWQIbeTCMikiq0RODuZcCtwChgNvCmu880swfM7JJgtVHAOjObBXwG3OXu68KKSb2GRETS1XWsoQZx9xHAiJR59yZ9dmJPKDfJU8pqIxARSZfpxuImlXiyWJlARCQhWokg+FdpQESkUrQSgdqKRUTSRCsRBGWCouKyDEciItJ8RCoRxOuGHv1oXmbjEBFpRiKVCGyXGzhbRCR80UoEmQ5ARKQZilQiEBGRdJFKBOosJCKSLlqJoCLTEYiIND+RSgRZrdRKICKSKlKJYM/WsaGVBvQ9OMORiIg0H5FKBAAd9siiTavIHbaISI0id0U0gwqNMSEikhC9RIDGGhIRSRa9RGCml9eLiCSJXCJoYVChPCAikhC5RLC2qITXxi3NdBgiIs1G5BKBiIhUFWoiMLN+ZjbXzPLMbFAt611pZm5mvcOMR0RE0oWWCMysJfAU0B/oCQwws57VrNceuB0YF1YsIiJSszBLBH2BPHdf6O4lwBvApdWs93vgYWB7iLGIiEgNwkwEXYFlSdP5wbwEMzsROMjdh9e2ITO7ycxyzSy3oKCg8SMVEYmwjDUWm1kL4FHglzta192fdffe7t47Ozs7/OBERCIkzESwHDgoabpbMC+uPdAL+NzMFgMnAcPUYCwi0rTCTAQTgB5m1t3MWgNXA8PiC919o7t3dvccd88BvgEucffcEGMSEZEUoSUCdy8DbgVGAbOBN919ppk9YGaXhLVfERGpn1ZhbtzdRwAjUubdW8O6Z4YZi4iIVC+yTxaXlOm9lSIiEOFEULi1JNMhiIg0C5FNBCIiEqNEICIScdFNBJbpAEREmofoJgK9nEZEBIhyIhARESDKiUBVQyIiQJQTgYiIAEoEIiKRF9lEMH91UaZDEBFpFiKbCGat2JTpEEREmoXIJgIREYlRIhARibjIJgJT91ERESDCicCb4Mnixz+ex98+XxD+jkREdkKoL6aJusc/ng/ALWceluFIRERqFtkSgaqGRERiIpsIREQkJtREYGb9zGyumeWZ2aBqlt9pZrPMbJqZfWJmh4QZT7IHP5jTVLsSEWnWQksEZtYSeAroD/QEBphZz5TVJgO93f1Y4G3gkbDiSVVeoXGoRUQg3BJBXyDP3Re6ewnwBnBp8gru/pm7bw0mvwG6hRiPiIhUI8xE0BVYljSdH8yryfXAByHGIyIi1WgWjcVm9mOgNzC4huU3mVmumeUWFBQ0bXCy07aWlJG3RoP8iTRXYSaC5cBBSdPdgnlVmNm5wD3AJe5eXN2G3P1Zd+/t7r2zs7MbLcAFBbo4NYUbXs7l3Ee/yHQYIlKDMBPBBKCHmXU3s9bA1cCw5BXM7ATg78SSwJoQY6nWOX/WxakpjF2wDgBvise5RaTeQksE7l4G3AqMAmYDb7r7TDN7wMwuCVYbDLQD3jKzKWY2rIbNSS3WbNrO74bNpKy8ItOh1EodtUSap1CHmHD3EcCIlHn3Jn0+N8z9R8Wv35vBx7NXc8YR2Zx11H6ZDqdGsRKBHukWaW6aRWNxc/XG+KXMWL4x02Ewb/XmWpePmR9rQB8+fWVThFNv8eE8VCIQaZ4ilwhO69G5TutNXrqBQe9O5+K/fBlyRLUbOWMl5z82muHTar7IF5fFqoQmLdnQVGE1iLP7ZoKy8goeHjmHjdtKMx2KSL1FLhHcfEbdRgK9/OmxIUdSN3NXFQX/7vjVms31MhuvDNqd24qHT1/J3z5fwEMfzM50KCL1FrlEcMrhVUsEk5Y277vouLpcQyua6ZXWgrqhZhpeoygrjx3c9tLm3WAvUp3IJYJUo+fV/wG1nEHD+ePwWSFEk65FcDtdl4toc73QJkoEzbbMsvPiY1epi6zsiiKfCNq0asnHs1ZTUla/O7nnxiwKKaKqKhtaG36B2VZSzs3/nMiKwm07Hc/ywm3kram98bom81bvvg/wPfFJ7CVEH89u8sdhRHZa5BPBxCXruWFILo9+NK/W9TZvLyVn0HA+nrW6TtttrNFNE9UqO7GNUTNXMXLmKh6qYejtgS+O57Bfj6h2WapTHvqUcx8dXa/9lwXn4ofPfVOv7+1KVm6MJdmi4rIMRyJNaVtJOTmDhvPgiF27bSjyiSB+B/f2xGW1rhcfK+cvn+XVabtF2xv3glCXAkFNpYZ4qaKmTXwxr6BJhuXeWlIe+j4yxfTKu0haWxQbFefvoxdmOJKdE/lEELe2qITlhdsYm7e2zt/559eLGTmjsltnSVkFc+rQu6c+Pp4dK4GMqMMzAjUli8rG2mjUX68rKmbUzFVNus/GSAPbSsqZuqywEbYUPndv9N+nf3y1iJxBw9myC5Wqdpf8r0SQ5Kz/+5wfPj+u9pWSfvl/+++Z3PzKpMT0A+/PpN/jY2J18Y3wC7K1pIzJS2MXhqXrt+5g7ZrVtftmfdtJGsLdQ+9rf/3Lufy/f05k49aG7+f9aSuYvbLuST35gtDQ83jnm1O49KmvWL+lpNrl703OZ/Wm7QBMyy/k1+9Nz1hy7373CLrfXbfqxLp64ctYu1tNxy/hUSJIEv8DTr37/mjW6kTVydT89CeNK4JluYtjXVELt5by/rQVieW/GzazQfH0vHdUvdZfXkNjcIvgKrWjJ4+HfL24Xvuri9SL4vNjFnHc/R82SsN1TeJJs7Si4Ynt1tcm0/+JMQ367rbS+leBFRWX8cGMVTV+f+O2Un4xdCrXvjAegB8/P47Xxi1l07Yd3z1PWLyetyfm1zumxjZmfgE5g4YnqlOauw1bShJP7ddkd6kSVCKoxs9enVRl+sYhuVz1zNc1rl9d3fw9781IfH5p7OJGi60hkn9X8zdsTdx5AYxftD7xeVo1SW5npZ6bD2etCuIIJxFsKylPlDiqu1leW1RMzqDhDJu6In1hNT6ZvbpJnjV5c0JlG1WLaq4t8ZuN1Zu313vb33/ma/7nrakNjq2xxH/vptfwe9bcai6ve2kC17wwnm21tG1V7CbjpkQyEVx3ck6jbi/+uxC/07nwyfrfSVZUeL2qImpSXVVB8nXlun9M4Pfvz2JNcEEZt3BdYll9q2zcnTfGL2V7Pe6A43dQYTVOH33vyMS2P5md3sMr3uj/ytdL6rS961/O5Yr6PmXegENLTtZWS71i/L+3IWfv87mZ7dqa+C+v4030mPkF9frdamzzgzG+JtdyI5B8U7Uri2QiuO97PRt1e/G73rVFDa/b/OtnefR/Yky1F6+4kTNWJgag21JcVu2dVXzsf4i1MSxau4UZKyrXi18I4xeU5O6OtZVyS8sr+NvnC6rMe3nsYga9O50/1dJ1bsz8qo3v8RJIY9dt5y5ez6kPf1plXnUXkRaJ7rhV9z9j+Ub+OHxWg+MqLd+542mRdPLDqm247h8TWLR2Szgbr8Hvhs3ko6DLdfzctqjDAc5asYlrXhjPA+83zYOb1YlX0dXWbhhGab+ouKzJSxqRTASNXa83Z9VmcgYNr3WdsvIKfvOv6azcuI0fPz+OnEHDeWTkHB4cMZt/TV6e6C1y/cu5NW7j5lcmcf5jo5mxfCO3vDqJ7/31y7Ri6/99OBeAq5/9mp73juKs//ucpz5bkLat+PWuMKlBtbY/0Fe/WcLDI6s+h/C7/8T+SONtI9W5cUj1x1NQVMz20vIqiWjz9lK+nF97r62xeWvJGTScwq0llJVXcP5jX/DRrNU8MnJuWnVTdX9K8UNctn5blTaV7z/zNc+NWcTWknL+PSXtRXpVfDEvVtdd0xvukpPMyBmrmJa/455A9yW1I31VTc+1LSWx87RxWyn3/2dm4kJRXFa/O+bLn/6qTustLCgid/H6Ha+4Ay+NXZz4HYjfFCT/lhVsLk6U4OL/N5u3l3Hti+MTcdTHyBkr+XjWan7fCAmkvleJnSnRb9xaynuT89m4tZRe943i8U/mU1HhjM1b2yQdAiKZCAC67bNHo23rsqd2/Mf11YJ1vPLNUga+OJ4vgz/0pz9fwN9HL+SOoVOqrDt1WWGt3Qgv/suXTA5GGv3tv2dUWRZPDN8srP2POF6KGZpbWTe9bksJpeUViUSV/Iu9pZZ60lkrN/FU8HzF5KUbeH5MrE91bcX629+YwvmPjabXfZUN4re+NpkfvzCOtUXFzFm1iUv/+iU5g4YzcsYq3spdRml5ReLubNLSDWzYWsq81UUMemdatXfRz3yxIO2CEK9/X7VpO6c89Gkixvj3y8qdx6p5uPDRD+cmujUOmxJrX7j7nenVVnElz7r5lYlc8teveGTknLQ/6Lw1RazetJ2XU+4q73wzvT7/87mVjZb/+Gpx4v+j758+SVu3NoVbS9laUsaW4jI2bS/l7nenUbi1hHcm5leJ7+w/f1Fru1h1Rs5YVaVxNbkrdfIDi/Ebjg1bSujzx48TA/XFE/lr45fU2KC8tqg4rdTs7sxYvpFl67dy8yuTuGFILi98uahKov987hpeHRerDswZNJxfVnOO3Z2vF6xLnIf6Xn77PzGGb5KqWuvj9qGT+cXQqYwPku9/pq7glXFL+OHz4zjx9x81aJv1EeqLaZqzuy44ktvfmLLjFRvJwOAOp6ZhFj6ZU1l/e2kdEsvm4KKU2htkzqq6Df+Qt6YorXfK1GWF9Ljng8R0/yfGsPihi6r9furb0AaPmsvPzzo8MWrrKYd33mGvm3jvHnfnrYn5fBGM+1RW7vR7vPK7N78yEYC73p6WmLe9tCKRzFq0MMYtSk98qzcV88KXi/jtxbGqwGn5hSxYU7Vq5KjfjuTjO89ITB/3wIfkdNozbVtPfppHfuE2HrziGBavi21j/OL1HPbrEdxz4dFV1q0uAT79+QJ+ef6RTFu2gUHvTOe9n59cr/c413WIkeKyck5+8FP+cFkvDu60J906ph/Lcfd/WKUqa+iEZVQ4dNwzi3OO7pJ2LP/z1lQWFGzhg9tPo6y8glMf/qzafcf/n+K/M8n/h8kJs0Vw+7lha6wq9aNZq/kqr/IC+so3SxOfp6TcEF37wnhmrdzEnN/3o21WSyBWmhpSTZvPEx/P45GrjgNi1WIA5/WMHd87k/IZePIhXPLXrxIx3/X2NN6emM/DVx7DD/ocnNZ4vWl7KXtktUwkspbVtOovXb+Vkw7tlH5ydmBxUGUXf/5l0dot/Cfo0LBhJ7pB11VkE8Fh2e0yHUJodlRNBSSK3jty8V/GcP8l32LwqLlV5idflOM+Shp+oz5dLz+ctZpfJW3vpAd3fJf7s1cnceWJ3YDqe9lUJ/5Hn2r8ovV1eur53UnLeXdSerXRH1PaSE5+6FOm3nc+x93/YZX5E5ds4OGRc5i7ejMzV9RejTB0wlLOOnI/Ji7ZQP9jDuAvn9b8RPuCgqLE73O/x8ewbksJt6T0fEuW2p5RkdJelJzIfjdsJu8H78IoK6/gkzlrWLWpsudSUXEZ7dpUvYzkDBrO+HvOqXH///36FN66+buJO+7F62p+RmZ7aQV3vDGZK7/djZMP68ysoJS6sGALPQ/cG6DaJACVQ5skV/Ull76Sfx/em5yfuKl6KzefPw6v+n86cckGrvzbWPr32p+xC9axZ+uWjB10dto+v5hXwMezVvP3a75dpQp647bSxO/DXRccyc/POrzK9+LnIPnGbkItVa6NzXa1p0179+7tubk116PXx/LCbZzy0Kc7XlFC1avr3sxY3rhPZCd7csAJXHLcgTUmyIP33XOnHthriL/+8ARufW1yndadeu/5HPfAh7WuM6DvQVx+Qjf+6+/1q85JdWSX9txxbo9qE8nt5/TgX1OWsyTlwj3pt+ex716tq5zfs4/aj0/nhNtLafJvz+O9yctrbVD++M7T6z02Vl09d23vGtvAXr/xJL57WCfG5q2lpGKyuwkAAAuHSURBVLyCN3OXMWJ65dPuix+6iNHzCup8Q1ZTybw+zGyiu/eudlmUEwHEGh9/P3x2o3TdFKmrrJa20z2NwjKg78G8Pn5p2vxzj+6SGPIk1Xe671tt9VyUHbV/+xqrau+58Oi0kuSOvPSTPpx5ZMPfSZ6xRGBm/YAngJbA8+7+UMryNsAQ4NvAOuAH7r64tm02diKIm71yU1p1Rv9e+yee9hQRybT3bzuVXl07NOi7tSWC0HoNmVlL4CmgP9ATGGBmqR34rwc2uPvhwGPAw2HFsyNHH7B3Wl3n2UftR+d2rTMUkYhIVc+GNMppmN1H+wJ57r7Q3UuAN4BLU9a5FHg5+Pw2cI5lcPCOF6/rQ9+cfenVdW++fcg+fL/3QfzntlO5/5JvMe7Xscavs47M5qNfnM7sB/rxzi3fZfBVx/Krfkembes3F8V6klx07AE8OeCEnYrrmR9/u051hEd0aXgD+MH7pvcuaWx3XZB+nnY3xzTwbk2kLjrumRXKdkOrGjKzq4B+7n5DMH0N8B13vzVpnRnBOvnB9IJgnbUp27oJuAng4IMP/vaSJXUbHqAprd60nXZtWtG6VQvcoXWrFkxdVshRB7SnTatYNzd3p7Tcmb9mM+uKSujUrjUFm4s588j9mJZfSPfOe7FHVktGzFjFmxOW8WXeWn5+1mHcdcFRQKznwfhF61m1cRsvf72ELcVlXHFiV3525uHsFZRmlq7bys2vTGTWyk28c8vJgDN82iouOvYACjZv59yjuzBvdRHT8gtZt6WEAzq0Zcz8tfz5+8fRIuh+8/WCdbRr04pxi9Zx8bEH8rNXJ7K8cBsnHrwPR+0fS5IHdGzLoZ334pkvFnLyYZ34w/BZiV4O2e3bULC5mNOPyOa7h3biwI5tOeXwznRu14btpeWs31LC4nVbGJu3jue/XMj3jj2Q047IZuqyQvbdqzW3nHEYM1Zs5JGRczm1R2ee/GQ+ZxyRzTUnHUKrli04NHsvOuyRxZrNxSxbv5Wi7WXcMCSXrh334DuH7suph3fmnKO70KZVC45/4MPEe4R7dd2b2885ghuH5PK3H53Ic2MWcmy3jtU+HXr2Ufvxp8uPocMeWXw0ezUfTF9J/oZttG/bKvH09j9+0oe735nOFSd25a2J+fzjuj706tqBm4bk8mHQg+rco/ej54EdOLJLex78YHaVh9722TOLu/sfza/emcYfL+/FSYd24qB99mT0vAJuqKERctJvz6OFwWdz17BqYzEHdmwLxMa2qu6lOK1btqCkvIIBfQ/mT5f34rO5a/jpS7m8cdNJTM/fyKdz1nD8wR3ZUlzG8Qd1rPIMQ/fOe7Fo7Rb+dPkxrCsq5s/B8xVH7d+e83p24ZYzD2PKskJ++Fz6k7c3ntad58Ys4vlre1NQVMxpPTozZ+VmOrdvwwEd2jJ0wjKy27fh7nenJ75zxQld+VbXDiwoKGJLcRl9u+/L8g3bePrzBVxxQlemLCuk6z57cHWfgxkxfSUTFq9nzebY8wZ3nncEeWuK6Nt9X3I67cUerVtwx9ApLFsfO99/uvwYtpaUMWrmqiq9cfp9a39+ftbhHLF/O/49eQW/eie9N9weWS1rHUjwvJ5duPWsw+l54N6cOfjzGgd/BDigQ1sKNhcnejTV5JTDO9GlfVuKistok9Uy0ZV0yr3n0XHPhtVSZKSNoDETQbKw2ghERHZnGWkjAJYDByVNdwvmVbuOmbUCOhBrNBYRkSYSZiKYAPQws+5m1hq4GhiWss4wYGDw+SrgU9/V+rOKiOziQnuy2N3LzOxWYBSx7qMvuvtMM3sAyHX3YcALwD/NLA9YTyxZiIhIEwp1iAl3HwGMSJl3b9Ln7cD3w4xBRERqF9nRR0VEJEaJQEQk4pQIREQiTolARCTidrnRR82sAGjoo8Wdgdrfhbj70znQOQCdA4jeOTjE3bOrW7DLJYKdYWa5NT1ZFxU6BzoHoHMAOgfJVDUkIhJxSgQiIhEXtUTwbKYDaAZ0DnQOQOcAdA4SItVGICIi6aJWIhARkRRKBCIiEReZRGBm/cxsrpnlmdmgTMfTmMzsRTNbE7zoJz5vXzP7yMzmB//uE8w3M3syOA/TzOzEpO8MDNafb2YDq9tXc2RmB5nZZ2Y2y8xmmtntwfwonYO2ZjbezKYG5+D+YH53MxsXHOvQYEh4zKxNMJ0XLM9J2tbdwfy5ZnZBZo6o4cyspZlNNrP3g+nInYN6c/fd/ofYMNgLgEOB1sBUoGem42rE4zsdOBGYkTTvEWBQ8HkQ8HDw+ULgA8CAk4Bxwfx9gYXBv/sEn/fJ9LHV8fgPAE4MPrcH5gE9I3YODGgXfM4CxgXH9iZwdTD/GeCW4PPPgGeCz1cDQ4PPPYO/jzZA9+DvpmWmj6+e5+JO4DXg/WA6cuegvj9RKRH0BfLcfaG7lwBvAJdmOKZG4+6jib3PIdmlwMvB55eBy5LmD/GYb4COZnYAcAHwkbuvd/cNwEdAv/Cj33nuvtLdJwWfNwOzga5E6xy4uxcFk1nBjwNnA28H81PPQfzcvA2cY2YWzH/D3YvdfRGQR+zvZ5dgZt2Ai4Dng2kjYuegIaKSCLoCy5Km84N5u7Mu7r4y+LwK6BJ8rulc7BbnKCjen0DsjjhS5yCoEpkCrCGWxBYAhe4ef6t98vEkjjVYvhHoxC5+DoDHgV8BFcF0J6J3DuotKokg0jxW3t3t+wmbWTvgHeAOd9+UvCwK58Ddy939eGLvB+8LHJXhkJqUmV0MrHH3iZmOZVcTlUSwHDgoabpbMG93tjqo7iD4d00wv6ZzsUufIzPLIpYEXnX3d4PZkToHce5eCHwGfJdYtVf8TYTJx5M41mB5B2Adu/Y5OAW4xMwWE6v+PRt4gmidgwaJSiKYAPQIeg+0JtYwNCzDMYVtGBDv9TIQ+HfS/GuDnjMnARuD6pNRwPlmtk/Qu+b8YF6zF9TrvgDMdvdHkxZF6Rxkm1nH4PMewHnE2ko+A64KVks9B/FzcxXwaVBqGgZcHfSo6Q70AMY3zVHsHHe/2927uXsOsb/xT939R0ToHDRYplurm+qHWE+RecTqTe/JdDyNfGyvAyuBUmL1mdcTq+v8BJgPfAzsG6xrwFPBeZgO9E7azk+JNYzlAT/J9HHV4/hPJVbtMw2YEvxcGLFzcCwwOTgHM4B7g/mHEruI5QFvAW2C+W2D6bxg+aFJ27onODdzgf6ZPrYGno8zqew1FMlzUJ8fDTEhIhJxUakaEhGRGigRiIhEnBKBiEjEKRGIiEScEoGISMQpEUhkmVlR8G+Omf2wkbf965TpsY25fZHGpEQgAjlAvRJB0pOqNamSCNz95HrGJNJklAhE4CHgNDObYma/CAZvG2xmE4L3Ffw/ADM708zGmNkwYFYw719mNjF4B8BNwbyHgD2C7b0azIuXPizY9gwzm25mP0ja9udm9raZzTGzV4MnpkVCt6O7GpEoGAT8j7tfDBBc0De6ex8zawN8ZWYfBuueCPTy2PDEAD919/XBsA4TzOwddx9kZrd6bAC4VFcAxwPHAZ2D74wOlp0AfAtYAXxFbOycLxv/cEWqUolAJN35xMYimkJsOOtOxMabARiflAQA/tvMpgLfEBuorAe1OxV43WMjha4GvgD6JG07390riA2TkdMoRyOyAyoRiKQz4DZ3rzLgnJmdCWxJmT4X+K67bzWzz4mNX9NQxUmfy9HfpzQRlQhEYDOxV1zGjQJuCYa2xsyOMLO9qvleB2BDkASOIvZqyLjS+PdTjAF+ELRDZBN7zejuPbKlNHu64xCJjdhZHlTxvERsDPscYFLQYFtA5esNk40Ebjaz2cRGqfwmadmzwDQzm+SxoZDj3iP2noCpxEZM/ZW7rwoSiUhGaPRREZGIU9WQiEjEKRGIiEScEoGISMQpEYiIRJwSgYhIxCkRiIhEnBKBiEjE/X+pDXu0MGJcswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}